# Model Settings
model_id: "microsoft/Phi-3-mini-4k-instruct"

# LoRA Parameters (Hyperparameter Tuning)
lora_r: 16                # Rank: Higher = more parameters to train
lora_alpha: 32            # Scaling factor (usually 2x Rank)
lora_dropout: 0.05
target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"] # Layers to adapt

# Training Hyperparameters
learning_rate: 0.0002     # 2e-4 is standard for LoRA
num_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
max_seq_length: 512

# Output & Monitoring
output_dir: "./outputs/financial-sentiment-model"
logging_steps: 10
save_steps: 100